{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AttentionTrader Data Analysis\n",
    "\n",
    "Basic analysis of the financial time series dataset collected from Yahoo Finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Data directory\n",
    "data_dir = Path('data/yfinance')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get all CSV files\n",
    "csv_files = list(data_dir.glob('*.csv'))\n",
    "print(f\"Total number of files: {len(csv_files)}\")\n",
    "print(f\"\\nFiles in dataset:\")\n",
    "for i, file in enumerate(sorted(csv_files), 1):\n",
    "    print(f\"{i:2d}. {file.stem}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load All Data and Analyze Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load all datasets and collect metadata\n",
    "datasets = {}\n",
    "metadata = []\n",
    "\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file, index_col=0, parse_dates=True)\n",
    "        ticker_name = file.stem\n",
    "        datasets[ticker_name] = df\n",
    "        \n",
    "        metadata.append({\n",
    "            'Ticker': ticker_name,\n",
    "            'Rows': len(df),\n",
    "            'Start Date': df.index.min(),\n",
    "            'End Date': df.index.max(),\n",
    "            'Days': (df.index.max() - df.index.min()).days,\n",
    "            'Columns': ', '.join(df.columns)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file.stem}: {e}\")\n",
    "\n",
    "# Create metadata DataFrame\n",
    "metadata_df = pd.DataFrame(metadata)\n",
    "metadata_df = metadata_df.sort_values('Days', ascending=False)\n",
    "\n",
    "print(f\"Successfully loaded {len(datasets)} datasets\")\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"Total rows across all files: {metadata_df['Rows'].sum():,}\")\n",
    "print(f\"Average rows per file: {metadata_df['Rows'].mean():.0f}\")\n",
    "print(f\"Median rows per file: {metadata_df['Rows'].median():.0f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display detailed metadata table\n",
    "print(\"\\nDetailed Dataset Information:\")\n",
    "print(\"=\" * 100)\n",
    "metadata_df.style.format({\n",
    "    'Rows': '{:,}',\n",
    "    'Days': '{:,}'\n",
    "})"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temporal Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Temporal statistics\n",
    "print(\"Temporal Coverage Statistics:\")\n",
    "print(f\"\\nEarliest data point: {metadata_df['Start Date'].min()}\")\n",
    "print(f\"Latest data point: {metadata_df['End Date'].max()}\")\n",
    "print(f\"\\nLongest history: {metadata_df.iloc[0]['Ticker']} ({metadata_df.iloc[0]['Days']:,} days)\")\n",
    "print(f\"Shortest history: {metadata_df.iloc[-1]['Ticker']} ({metadata_df.iloc[-1]['Days']:,} days)\")\n",
    "print(f\"\\nAverage coverage: {metadata_df['Days'].mean():.0f} days ({metadata_df['Days'].mean()/365:.1f} years)\")\n",
    "print(f\"Median coverage: {metadata_df['Days'].median():.0f} days ({metadata_df['Days'].median()/365:.1f} years)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Data Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot timeline of data coverage\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Sort by start date for better visualization\n",
    "plot_df = metadata_df.sort_values('Start Date')\n",
    "\n",
    "for idx, row in plot_df.iterrows():\n",
    "    ax.barh(row['Ticker'], \n",
    "            width=(row['End Date'] - row['Start Date']).days,\n",
    "            left=row['Start Date'],\n",
    "            height=0.7,\n",
    "            alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Ticker', fontsize=12)\n",
    "ax.set_title('Historical Data Coverage Timeline', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Distribution of data points\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of number of rows\n",
    "axes[0].hist(metadata_df['Rows'], bins=20, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Number of Rows', fontsize=11)\n",
    "axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0].set_title('Distribution of Dataset Sizes', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram of time coverage in years\n",
    "axes[1].hist(metadata_df['Days']/365, bins=20, alpha=0.7, color='coral', edgecolor='black')\n",
    "axes[1].set_xlabel('Years of Data', fontsize=11)\n",
    "axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1].set_title('Distribution of Temporal Coverage', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Top 10 Datasets by Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Top 10 by number of days\n",
    "top_10_days = metadata_df.nlargest(10, 'Days')[['Ticker', 'Days', 'Rows', 'Start Date', 'End Date']].copy()\n",
    "top_10_days['Years'] = (top_10_days['Days'] / 365).round(1)\n",
    "\n",
    "print(\"Top 10 Datasets by Historical Coverage:\")\n",
    "print(top_10_days.to_string(index=False))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Top 10 by number of rows\n",
    "top_10_rows = metadata_df.nlargest(10, 'Rows')[['Ticker', 'Rows', 'Days', 'Start Date', 'End Date']].copy()\n",
    "top_10_rows['Years'] = (top_10_rows['Days'] / 365).round(1)\n",
    "\n",
    "print(\"Top 10 Datasets by Number of Data Points:\")\n",
    "print(top_10_rows.to_string(index=False))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample Data Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Examine a sample dataset (Apple)\n",
    "if 'apple' in datasets:\n",
    "    apple_df = datasets['apple']\n",
    "    \n",
    "    print(\"Apple (AAPL) Dataset Sample:\")\n",
    "    print(f\"\\nShape: {apple_df.shape}\")\n",
    "    print(f\"\\nColumns: {list(apple_df.columns)}\")\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    display(apple_df.head())\n",
    "    print(f\"\\nLast 5 rows:\")\n",
    "    display(apple_df.tail())\n",
    "    print(f\"\\nBasic Statistics:\")\n",
    "    display(apple_df.describe())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for missing values across all datasets\n",
    "missing_data = []\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    missing_count = df.isnull().sum().sum()\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    missing_pct = (missing_count / total_cells) * 100 if total_cells > 0 else 0\n",
    "    \n",
    "    missing_data.append({\n",
    "        'Ticker': name,\n",
    "        'Missing Values': missing_count,\n",
    "        'Total Cells': total_cells,\n",
    "        'Missing %': round(missing_pct, 2)\n",
    "    })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_data)\n",
    "missing_df = missing_df.sort_values('Missing %', ascending=False)\n",
    "\n",
    "print(\"Data Quality Summary:\")\n",
    "print(f\"\\nDatasets with missing values: {(missing_df['Missing Values'] > 0).sum()}\")\n",
    "print(f\"Total missing values across all datasets: {missing_df['Missing Values'].sum():,}\")\n",
    "\n",
    "if (missing_df['Missing Values'] > 0).any():\n",
    "    print(\"\\nDatasets with missing values:\")\n",
    "    print(missing_df[missing_df['Missing Values'] > 0].to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nâœ“ No missing values found in any dataset!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Category Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Categorize tickers by type\n",
    "categories = {\n",
    "    'Big Tech': ['apple', 'microsoft', 'alphabet', 'nvidia', 'meta', 'amazon'],\n",
    "    'Finance': ['jpmorgan', 'visa', 'berkshire_hathaway'],\n",
    "    'Healthcare': ['unitedhealth', 'johnson_and_johnson', 'pfizer'],\n",
    "    'Consumer': ['tesla', 'mcdonalds', 'walmart', 'coca_cola', 'procter_gamble'],\n",
    "    'Energy': ['exxonmobil', 'chevron'],\n",
    "    'Industrials': ['caterpillar', 'union_pacific', 'boeing'],\n",
    "    'Utilities': ['nextera_energy', 'duke_energy'],\n",
    "    'Real Estate': ['prologis', 'american_tower'],\n",
    "    'Materials': ['linde', 'freeport_mcmoran'],\n",
    "    'Telecom': ['verizon', 'tmobile'],\n",
    "    'International': ['taiwan_semiconductor', 'asml', 'toyota', 'alibaba'],\n",
    "    'Crypto': ['bitcoin', 'ethereum', 'solana'],\n",
    "    'Indices': ['sp500', 'nasdaq100', 'dow_jones', 'russell2000', 'vix_volatility']\n",
    "}\n",
    "\n",
    "category_stats = []\n",
    "for category, tickers in categories.items():\n",
    "    ticker_data = metadata_df[metadata_df['Ticker'].isin(tickers)]\n",
    "    category_stats.append({\n",
    "        'Category': category,\n",
    "        'Count': len(ticker_data),\n",
    "        'Avg Days': ticker_data['Days'].mean(),\n",
    "        'Avg Rows': ticker_data['Rows'].mean(),\n",
    "        'Total Rows': ticker_data['Rows'].sum()\n",
    "    })\n",
    "\n",
    "category_df = pd.DataFrame(category_stats)\n",
    "category_df = category_df.sort_values('Total Rows', ascending=False)\n",
    "\n",
    "print(\"Dataset Breakdown by Category:\")\n",
    "print(category_df.to_string(index=False))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize category distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Number of tickers per category\n",
    "category_df.plot(x='Category', y='Count', kind='barh', ax=axes[0], \n",
    "                  color='steelblue', legend=False)\n",
    "axes[0].set_xlabel('Number of Tickers', fontsize=11)\n",
    "axes[0].set_ylabel('Category', fontsize=11)\n",
    "axes[0].set_title('Tickers per Category', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Total data points per category\n",
    "category_df.plot(x='Category', y='Total Rows', kind='barh', ax=axes[1], \n",
    "                  color='coral', legend=False)\n",
    "axes[1].set_xlabel('Total Data Points', fontsize=11)\n",
    "axes[1].set_ylabel('Category', fontsize=11)\n",
    "axes[1].set_title('Total Data Points per Category', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a comprehensive overview of the AttentionTrader dataset:\n",
    "- Dataset coverage and temporal analysis\n",
    "- Data quality assessment\n",
    "- Category distribution\n",
    "- Visual representations of the data structure\n",
    "\n",
    "The dataset is ready for further analysis including:\n",
    "- Price trend analysis\n",
    "- Volatility studies\n",
    "- Correlation analysis\n",
    "- Feature engineering for machine learning models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
